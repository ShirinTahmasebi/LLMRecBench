name: 'LLMRec'

# LLaMa Config
no_checkpoint_model_name: 'meta-llama/Llama-2-7b-chat-hf'
no_checkpoint_model_short_name: 'llama2-7b'
temperature: 0.1
top_p: 0.75
top_k: 10
max_tokens: 500

# Solution Details
number_of_history_items: 10
number_of_candidates: -1
ground_truth_position: -1
number_of_recommendations: 10

# Paths
checkpoint_model_name: 'baffo32/decapoda-research-llama-7B-hf'
checkpoint_path: 'checkpoints/llmrec'

# Finetuning Args
num_of_users_to_train: 4000
lora_alpha: 16
lora_dropout: .1
lora_r: 64
training_args_train_batch_size: 4 # Num of steps per epoch: dataset size / batch_size
training_args_eval_batch_size: 4
training_args_learning_rate: 3e-4
training_args_logging_steps: 2
training_args_max_steps: 500
training_args_max_sequence_len: 500
