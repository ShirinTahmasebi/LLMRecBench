# LLaMa Config
model_name: 'meta-llama/Llama-2-7b-chat-hf'
model_short_name: 'llama2-7b'
temperature: 1
top_p: 0.75
top_k: 10
max_tokens: 500

# Solution Details
number_of_history_items: 10
number_of_candidates: -1
ground_truth_position: -1
number_of_recommendations: 5

# Paths
lora_weights_path: 'checkpoints/genrec'
checkpoint_model_name: 'baffo32/decapoda-research-llama-7B-hf'